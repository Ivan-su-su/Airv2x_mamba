########
# Name #
########

name: airv2x_intermediate_cobevt

############
# Training #
############

active_sensors: &active_sensors ["lidar"]
collaborators: &collaborators ["vehicle", "rsu", "drone"]
ego_type: &ego_type vehicle
bevcam_fov: 110

yaml_parser: "load_airv2x_params" # add support for airv2x
train_params:
  batch_size: &batch_size 1
  epoches: 50
  eval_freq: 2
  save_freq: 1
  max_cav: &max_cav
    vehicle: 3
    rsu: 2
    drone: 2

device: &device cuda
train: &train True


###########
# Dataset #
###########

root_dir: 'dataset/airv2x/train'
validate_dir: 'dataset/airv2x/val'
test_dir: 'dataset/airv2x/test'
num_class: &num_class 7
num_anchor: &num_anchor 2

task: &task "det" # "det" or "seg"
seg_branch: &seg_branch "both"
seg_hw: &seg_hw 512 # resolution of the segmentation map
seg_res: &seg_res 0.25
dynamic_class: &dynamic_class 7
static_class: &static_class 3

fusion:
  core_method: 'IntermediateFusionDatasetAirv2x' # LateFusionDataset, EarlyFusionDataset, IntermediateFusionDataset supported
  args:
    proj_first: &proj_first true
    veh_grid_conf: &veh_grid_conf
      xbound: [-140.8, 140.8, 0.4]    
      ybound: [-40, 40, 0.4]  
      zbound: [-10, 10, 20.0] 
      ddiscr: [2, 50, 48]
      mode: 'LID'
    veh_data_aug_conf: &veh_data_aug_conf_image
      resize_lim: [0.65, 0.7]
      final_dim: [360, 640]
      rot_lim: [0, 0]
      H: 720 # 720
      W: 1280 # 1280
      rand_flip: False
      bot_pct_lim: [0.0, 0.05]
    
    rsu_grid_conf: &rsu_grid_conf
      xbound: [-140.8, 140.8, 0.4]  
      ybound: [-40, 40, 0.4] 
      zbound: [-30, 30, 60.0] 
      ddiscr: [2, 50, 48]
      mode: 'LID'
    rsu_data_aug_conf: &rsu_data_aug_conf_image
      resize_lim: [0.65, 0.7]
      final_dim: [360, 640]
      rot_lim: [0, 0]
      H: 720 # 720
      W: 1280 # 1280
      rand_flip: False
      bot_pct_lim: [0.0, 0.05]

    drone_grid_conf: &drone_grid_conf
      xbound: [-140.8, 140.8, 0.4]  
      ybound: [-40, 40, 0.4] 
      zbound: [-150, -6, 144] 
      ddiscr: [6, 150, 144]
      mode: 'UD'
    drone_data_aug_conf: &drone_data_aug_conf_image
      resize_lim: [0.65, 0.7]
      final_dim: [360, 640]
      rot_lim: [-3.6, 3.6]
      H: 720 # 720
      W: 1280 # 1280
      rand_flip: False
      bot_pct_lim: [0.0, 0.05]
    
# Note: data augmentation is not used for intermediate fusion, but we still keep the data augmentation config for consistency.
data_augment:
  - NAME: random_world_flip
    ALONG_AXIS_LIST: [ 'x' ]

  - NAME: random_world_rotation
    WORLD_ROT_ANGLE: [ -0.78539816, 0.78539816 ]

  - NAME: random_world_scaling
    WORLD_SCALE_RANGE: [ 0.95, 1.05 ]

# preprocess-related
preprocess:
  # options: BasePreprocessor, VoxelPreprocessor, BevPreprocessor
  core_method: 'SpVoxelPreprocessor'
  ego_type: *ego_type
  args:
    voxel_size: &voxel_size [0.4, 0.4, 4] # Use ego type
    max_points_per_voxel: 32
    max_voxel_train: 32000
    max_voxel_test: 70000
  # lidar range for each agent
  cav_lidar_range: &cav_lidar [-140.8, -40, -3, 140.8, 40, 1] # Use ego type    

# anchor box related
postprocess:
  core_method: 'VoxelPostprocessor' # VoxelPostprocessor, BevPostprocessor supported
  ego_type: *ego_type
  anchor_args:
    cav_lidar_range: *cav_lidar # Note: Use ego type
    l: 3.9
    w: 1.6
    h: 1.56
    r: [0, 90]
    feature_stride: 2
    num: *num_anchor
  target_args:
    pos_threshold: 0.60
    neg_threshold: 0.45
    score_threshold: 0.20
    obj_threshold: 0.20
  order: 'hwl' # hwl or lwh
  max_num: 300 # maximum number of objects in a single frame. use this number to make sure different frames has the same dimension in the same batch
  nms_thresh: 0.15

#########
# Model #
#########

# model related
model:
  core_method: airv2x_cobevt
  args:
    collaborators: *collaborators
    active_sensors: *active_sensors
    max_cav: *max_cav
    device: *device
    train: *train
    proj_first: *proj_first
    supervise_single: False
    backbone_fix: False
    

    ###################
    # Vehicular Agent #
    ###################
    vehicle:
      modalities: ["lidar"]  # "cam", "lidar"

      lidar:
        voxel_size: &veh_voxel_size [0.4, 0.4, 4]
        lidar_range: &veh_lidar [-140.8, -40, -3, 140.8, 40, 1]
        compression: 0 # compression rate
        backbone_fix: false
        pillar_vfe:
          use_norm: true
          with_distance: false
          use_absolute_xyz: true
          num_filters: [64]
        point_pillar_scatter:
          num_features: 64

      cam:
        grid_conf: *veh_grid_conf
        data_aug_conf: *veh_data_aug_conf_image
        img_downsample: 8
        img_features: 64
        bevout_feature: 64
        camera_encoder: EfficientNet
        use_depth_gt: true
        depth_supervision: false
    
    ###################
    # Road Side Units #
    ###################

    rsu:
      modalities: ["lidar"]  # "cam", "lidar"

      lidar:
        voxel_size: &rsu_voxel_size [0.4, 0.4, 60]
        lidar_range: &rsu_lidar [-140.8, -40, -30, 140.8, 40, 30]
        compression: 0 # compression rate
        backbone_fix: false
        pillar_vfe:
          use_norm: true
          with_distance: false
          use_absolute_xyz: true
          num_filters: [64]
        point_pillar_scatter:
          num_features: 64

      cam:
        grid_conf: *rsu_grid_conf
        data_aug_conf: *rsu_data_aug_conf_image
        img_downsample: 8
        img_features: 64
        bevout_feature: 64
        camera_encoder: EfficientNet
        use_depth_gt: true
        depth_supervision: false

    ######################
    # UAV (Drone) Agents #
    ######################

    drone:
      modalities: ["lidar"]  # "cam", "lidar"

      lidar:
        voxel_size: &drone_voxel_size [0.4, 0.4, 144]
        lidar_range: &drone_lidar [-140.8, -40, -150, 140.8, 40, -6]
        compression: 0 # compression rate
        backbone_fix: false
        pillar_vfe:
          use_norm: true
          with_distance: false
          use_absolute_xyz: true
          num_filters: [64]
        point_pillar_scatter:
          num_features: 64

      cam:
        grid_conf: *drone_grid_conf
        data_aug_conf: *drone_data_aug_conf_image
        img_downsample: 8
        img_features: 64
        bevout_feature: 64
        camera_encoder: EfficientNet
        use_depth_gt: true
        depth_supervision: false

    base_bev_backbone:
      layer_nums: [3, 5, 8]
      layer_strides: [2, 2, 2]
      num_filters: [64, 128, 256]
      upsample_strides: [1, 2, 4]
      num_upsample_filter: [128, 128, 128]
    
    shrink_header:
      use: true 
      input_dim: 384  # 128 * 3
      dim: [ &out_channels 256 ] 
      kernal_size: [ 1 ] # YH: we don't shrink the HW in the current version
      stride: [ 1 ]
      padding: [ 0 ]

    compression: 0 # compression rate

    fax_fusion:
      input_dim: 256
      mlp_dim: 256
      window_size: 4
      dim_head: 32
      drop_out: 0.1
      depth: 3
      mask: true

    # params for prediction
    task: *task
    seg_branch: *seg_branch
    seg_hw: *seg_hw
    head_dim: 256
    outC: *out_channels
    anchor_number: *num_anchor
    num_class: *num_class
    dynamic_class: *dynamic_class
    static_class: *static_class
    cav_range: *cav_lidar
    seg_res: *seg_res
    obj_head: true

##################
# Loss Functions #
##################

loss:
  det:
    core_method: point_pillar_loss_multiclass
    args:
      cls_weight: 1.0
      reg: 2.0
      num_class: *num_class
  seg:
    core_method: vanilla_seg_loss
    args:
      seg_branch: *seg_branch
      d_weights:
        - 200.0
        - 200.0
        - 75.0
        - 200.0
        - 200.0
        - 200.0
      s_weights: 50.0
      l_weights: 8.0
      d_coe: 2.0
      s_coe: 0.0

###########
# Learner #
###########

optimizer:
  core_method: Adam
  lr: 0.002
  args:
    eps: 1e-10
    weight_decay: 1e-4

lr_scheduler:
  core_method: multistep #step, multistep and Exponential support
  gamma: 0.1
  step_size: [10, 15]

